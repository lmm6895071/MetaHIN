{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import re\r\n",
    "import random\r\n",
    "import pickle\r\n",
    "import os\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "input_dir = 'original/'\r\n",
    "output_dir = './'\r\n",
    "melu_output_dir = '../../../MeLU/yelp/'\r\n",
    "states = [ \"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]\r\n",
    "\r\n",
    "if not os.path.exists(\"{}/meta_training/\".format(output_dir)):\r\n",
    "    os.mkdir(\"{}/log/\".format(output_dir))\r\n",
    "    for state in states:\r\n",
    "        os.mkdir(\"{}/{}/\".format(output_dir, state))\r\n",
    "        os.mkdir(\"{}/{}/\".format(melu_output_dir, state))\r\n",
    "        if not os.path.exists(\"{}/{}/{}\".format(output_dir, \"log\", state)):\r\n",
    "            os.mkdir(\"{}/{}/{}\".format(output_dir, \"log\", state))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "ui_data = pd.read_csv(input_dir+'rating.dat', names=['user', 'item', 'rating', 'timestamp'],sep=\"\\t\", engine='python')\r\n",
    "len(ui_data)  # 1302630"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1302630"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "user_list = set(ui_data.user)\r\n",
    "item_list = set(ui_data.item)\r\n",
    "len(user_list), len(item_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(51670, 34259)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "new_user = pd.read_csv(input_dir+'new_user.dat', names=['user'], engine='python').user.tolist()\r\n",
    "new_item = pd.read_csv(input_dir+'new_item.dat', names=['item'], engine='python').item.tolist()\r\n",
    "len(new_user), len(new_item)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10604, 6851)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "exist_user = list(user_list - set(new_user))\r\n",
    "exist_item = list(item_list- set(new_item))\r\n",
    "len(exist_user), len(exist_item)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(41066, 27408)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# meta-training and meta-testing data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "meta_training_data = ui_data[(ui_data['user'].isin(exist_user)) & (ui_data['item'].isin(exist_item))]\r\n",
    "len(meta_training_data)  # 967291 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "967291"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "warm_data = meta_training_data.sample(int(0.1*len(meta_training_data)))\r\n",
    "len(warm_data)  # 96729 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "96729"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# warm data\r\n",
    "warm_x = {k: g[\"item\"].tolist() for k,g in warm_data.groupby(\"user\")}\r\n",
    "warm_y = {k: g[\"rating\"].tolist() for k,g in warm_data.groupby(\"user\")}\r\n",
    "json.dump(warm_x, open(\"{}/warm_up.json\".format(output_dir), 'w'))\r\n",
    "json.dump(warm_y, open(\"{}/warm_up_y.json\".format(output_dir), 'w'))\r\n",
    "len(warm_x), len(warm_y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32145, 32145)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "count_user_interaction = dict(warm_data.user.value_counts())\r\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\r\n",
    "len(less_100_user)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "training_data = meta_training_data.loc[meta_training_data.index.difference(warm_data.index)]\r\n",
    "len(training_data)  # 870562 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "870562"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# training data\r\n",
    "training_x = {k: g[\"item\"].tolist() for k,g in training_data.groupby(\"user\")}\r\n",
    "training_y = {k: g[\"rating\"].tolist() for k,g in training_data.groupby(\"user\")}\r\n",
    "json.dump(training_x, open(\"{}/meta_training.json\".format(output_dir), 'w'))\r\n",
    "json.dump(training_y, open(\"{}/meta_training_y.json\".format(output_dir), 'w'))\r\n",
    "len(training_x), len(training_y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(41066, 41066)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "count_user_interaction = dict(training_data.user.value_counts())\r\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\r\n",
    "len(less_100_user)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20635"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# meta-testing\r\n",
    "# user_cold_testing\r\n",
    "user_cold_data =ui_data[(ui_data['user'].isin(new_user)) & (ui_data['item'].isin(exist_item))]\r\n",
    "len(user_cold_data)  # 164136 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "164136"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "user_cold_x = {k: g[\"item\"].tolist() for k,g in user_cold_data.groupby(\"user\")}\r\n",
    "user_cold_y = {k: g[\"rating\"].tolist() for k,g in user_cold_data.groupby(\"user\")}\r\n",
    "json.dump(user_cold_x, open(\"{}/user_cold_testing.json\".format(output_dir), 'w'))\r\n",
    "json.dump(user_cold_y, open(\"{}/user_cold_testing_y.json\".format(output_dir), 'w'))\r\n",
    "len(user_cold_x), len(user_cold_y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10603, 10603)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "count_user_interaction = dict(user_cold_data.user.value_counts())\r\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\r\n",
    "len(less_100_user)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4334"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# item_cold_testing\r\n",
    "item_cold_data =ui_data[(ui_data['user'].isin(exist_user)) & (ui_data['item'].isin(new_item))]\r\n",
    "len(item_cold_data)  # 118467 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "118467"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "item_cold_x = {k: g[\"item\"].tolist() for k,g in item_cold_data.groupby(\"user\")}\r\n",
    "item_cold_y = {k: g[\"rating\"].tolist() for k,g in item_cold_data.groupby(\"user\")}\r\n",
    "json.dump(item_cold_x, open(\"{}/item_cold_testing.json\".format(output_dir), 'w'))\r\n",
    "json.dump(item_cold_y, open(\"{}/item_cold_testing_y.json\".format(output_dir), 'w'))\r\n",
    "len(item_cold_x), len(item_cold_y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(25578, 25578)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "count_user_interaction = dict(item_cold_data.user.value_counts())\r\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\r\n",
    "len(less_100_user)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# user_and_item_cold_testing\r\n",
    "user_item_cold_data =ui_data[(ui_data['user'].isin(new_user)) & (ui_data['item'].isin(new_item))]\r\n",
    "len(user_item_cold_data)  # 52736 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "52736"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "user_item_cold_x = {k: g[\"item\"].tolist() for k,g in user_item_cold_data.groupby(\"user\")}\r\n",
    "user_item_cold_y = {k: g[\"rating\"].tolist() for k,g in user_item_cold_data.groupby(\"user\")}\r\n",
    "json.dump(user_item_cold_x, open(\"{}/user_and_item_cold_testing.json\".format(output_dir), 'w'))\r\n",
    "json.dump(user_item_cold_y, open(\"{}/user_and_item_cold_testing_y.json\".format(output_dir), 'w'))\r\n",
    "len(user_item_cold_x), len(user_item_cold_y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(9655, 9655)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "count_user_interaction = dict(user_item_cold_data.user.value_counts())\r\n",
    "less_10_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\r\n",
    "len(less_10_user)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "len(training_data)+len(warm_data)+len(user_cold_data)+len(item_cold_data)+len(user_item_cold_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1302630"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# support set and query set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. user and item feature"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "user_fans = pd.read_csv(input_dir+'user_fans.dat', names=['user','fans'], sep='\\t', engine='python')\r\n",
    "# user_fans\r\n",
    "len(user_fans)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "user_avgrating = pd.read_csv(input_dir+'user_avgrating.dat', names=['user','avgrating'], sep='\\t', engine='python')\r\n",
    "len(user_avgrating)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "user_friends = pd.read_csv(input_dir+'user_friends.dat', names=['user','friends'], sep='\\t', engine='python')\r\n",
    "len(user_friends)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "item_stars =  pd.read_csv(input_dir+'item_stars.dat', names=['item','stars'], sep='\\t', engine='python')\r\n",
    "len(item_stars)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "item_postalcode =  pd.read_csv(input_dir+'item_postalcode.dat', names=['item','postalcode'], sep='\\t', engine='python')\r\n",
    "len(item_postalcode)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "item_city =  pd.read_csv(input_dir+'item_city.dat', names=['item','city'], sep='\\t', engine='python')\r\n",
    "len(item_city)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "item_city.city.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "63     4614\n",
       "328    4511\n",
       "108    2650\n",
       "24     1735\n",
       "151    1699\n",
       "       ... \n",
       "107       1\n",
       "75        1\n",
       "43        1\n",
       "25        1\n",
       "511       1\n",
       "Name: city, Length: 513, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "item_category =  pd.read_csv(input_dir+'item_category.dat', names=['item','category'], sep='\\t', engine='python')\r\n",
    "len(item_category)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "b_types = defaultdict(set)\r\n",
    "for index, row in item_category.iterrows():\r\n",
    "    types = list(map(int, row['category'].strip().split(' ')))\r\n",
    "    b_types[row['item']].update(types)\r\n",
    "b_types = dict(b_types)\r\n",
    "t_businesses = reverse_dict(b_types)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sorted([len(v) for k, v in t_businesses.items()], reverse=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[34259,\n",
       " 6509,\n",
       " 5594,\n",
       " 5399,\n",
       " 4507,\n",
       " 3710,\n",
       " 3489,\n",
       " 3164,\n",
       " 3162,\n",
       " 3120,\n",
       " 3014,\n",
       " 2994,\n",
       " 2797,\n",
       " 2613,\n",
       " 1748,\n",
       " 1712,\n",
       " 1597,\n",
       " 1511,\n",
       " 1457,\n",
       " 1447,\n",
       " 1285,\n",
       " 1277,\n",
       " 1253,\n",
       " 1171,\n",
       " 1081,\n",
       " 1054,\n",
       " 1030,\n",
       " 1023,\n",
       " 1008,\n",
       " 946,\n",
       " 927,\n",
       " 898,\n",
       " 895,\n",
       " 889,\n",
       " 856,\n",
       " 846,\n",
       " 817,\n",
       " 730,\n",
       " 686,\n",
       " 651,\n",
       " 633,\n",
       " 630,\n",
       " 627,\n",
       " 626,\n",
       " 626,\n",
       " 575,\n",
       " 553,\n",
       " 547,\n",
       " 529,\n",
       " 510,\n",
       " 504,\n",
       " 495,\n",
       " 466,\n",
       " 465,\n",
       " 438,\n",
       " 428,\n",
       " 422,\n",
       " 419,\n",
       " 378,\n",
       " 371,\n",
       " 348,\n",
       " 321,\n",
       " 313,\n",
       " 306,\n",
       " 300,\n",
       " 272,\n",
       " 259,\n",
       " 253,\n",
       " 251,\n",
       " 251,\n",
       " 244,\n",
       " 234,\n",
       " 234,\n",
       " 233,\n",
       " 213,\n",
       " 210,\n",
       " 209,\n",
       " 194,\n",
       " 194,\n",
       " 193,\n",
       " 191,\n",
       " 179,\n",
       " 169,\n",
       " 160,\n",
       " 157,\n",
       " 157,\n",
       " 155,\n",
       " 154,\n",
       " 154,\n",
       " 146,\n",
       " 144,\n",
       " 142,\n",
       " 137,\n",
       " 136,\n",
       " 136,\n",
       " 134,\n",
       " 134,\n",
       " 133,\n",
       " 133,\n",
       " 121,\n",
       " 120,\n",
       " 116,\n",
       " 111,\n",
       " 108,\n",
       " 107,\n",
       " 107,\n",
       " 105,\n",
       " 95,\n",
       " 93,\n",
       " 90,\n",
       " 85,\n",
       " 83,\n",
       " 82,\n",
       " 80,\n",
       " 79,\n",
       " 75,\n",
       " 74,\n",
       " 70,\n",
       " 69,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 64,\n",
       " 61,\n",
       " 59,\n",
       " 58,\n",
       " 58,\n",
       " 57,\n",
       " 56,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 53,\n",
       " 53,\n",
       " 52,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 49,\n",
       " 48,\n",
       " 47,\n",
       " 47,\n",
       " 46,\n",
       " 46,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 42,\n",
       " 41,\n",
       " 41,\n",
       " 40,\n",
       " 39,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 36,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 34,\n",
       " 34,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 31,\n",
       " 31,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 28,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "item_state =  pd.read_csv(input_dir+'item_state.dat', names=['item','state'], sep='\\t', engine='python')\r\n",
    "len(item_state)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "item_reviewcount =  pd.read_csv(input_dir+'item_reviewcount.dat', names=['item','reviewcount'], sep='\\t', engine='python')\r\n",
    "len(item_reviewcount)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "item_neighbor =  pd.read_csv(input_dir+'item_neighbor.dat', names=['item','neighbor'], sep='\\t', engine='python')\r\n",
    "len(item_neighbor)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "list(item_category[item_category['item']==1].category)[0].strip().split(' ')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['102', '22', '331']"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "len(set(item_category.category))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "18174"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "user_fea = {}\r\n",
    "for i in tqdm(user_list):\r\n",
    "    fans_idx = list(user_fans[user_fans['user']==i].fans)[0]\r\n",
    "    fans = torch.tensor([[fans_idx]]).long()\r\n",
    "    avgrating_idx = list(user_avgrating[user_avgrating['user']==i].avgrating)[0]\r\n",
    "    avgrating = torch.tensor([[avgrating_idx]]).long()\r\n",
    "    \r\n",
    "    user_fea[i] = torch.cat((fans, avgrating),1)\r\n",
    "len(user_fea)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 51670/51670 [01:51<00:00, 462.80it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "np.save(output_dir+'user_feature.npy',user_fea)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "item_fea_homo = {}\n",
    "item_fea_hete = {}\n",
    "for i in tqdm(item_list):\n",
    "    stars_idx = list(item_stars[item_stars['item']==i].stars)[0]\n",
    "    stars = torch.tensor([[stars_idx]]).long()\n",
    "    postalcode_idx = list(item_postalcode[item_postalcode['item']==i].postalcode)[0]\n",
    "    postalcode = torch.tensor([[postalcode_idx]]).long()\n",
    "    reviewcount_idx = list(item_reviewcount[item_reviewcount['item']==i].reviewcount)[0]\n",
    "    reviewcount = torch.tensor([[reviewcount_idx]]).long()\n",
    "    \n",
    "    city_idx = list(item_city[item_city['item']==i].city)[0]\n",
    "    city = torch.tensor([[city_idx]]).long()\n",
    "    state_idx = list(item_state[item_state['item']==i].state)[0]\n",
    "    state = torch.tensor([[state_idx]]).long()\n",
    "    \n",
    "#     category = torch.zeros(1, 542).long()\n",
    "#     categories = list(item_category[item_category['item']==i].category)[0].strip().split(' ')\n",
    "#     for c in categories:\n",
    "#         category[0, int(c)] = 1\n",
    "#     item_fea_hete[i] = torch.cat((stars, postalcode,reviewcount, category),1)\n",
    "#     item_fea_homo[i] = torch.cat((stars, postalcode, reviewcount, city, state, category), 1)\n",
    "    \n",
    "    item_fea_hete[i] = torch.cat((stars, postalcode,reviewcount),1)\n",
    "    item_fea_homo[i] = torch.cat((stars, postalcode, reviewcount, city, state), 1)\n",
    "len(item_fea_hete), len(item_fea_homo)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 34259/34259 [03:41<00:00, 154.99it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(34259, 34259)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.save(output_dir+'item_feature_hete.npy',item_fea_hete)\n",
    "np.save(output_dir+'item_feature_homo.npy',item_fea_homo)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. mp data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "states = [ \"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "    import collections\r\n",
    "    from collections import defaultdict\r\n",
    "    def reverse_dict(d):\r\n",
    "        # {1:[a,b,c], 2:[a,f,g],...}\r\n",
    "        re_d = collections.defaultdict(list)\r\n",
    "        for k, v_list in d.items():\r\n",
    "            for v in v_list:\r\n",
    "                re_d[v].append(k)\r\n",
    "        return dict(re_d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "tqdm._instances.clear()\r\n",
    "del tqdm\r\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "b_cities = {k: g[\"city\"].tolist() for k,g in item_city.groupby(\"item\")}\r\n",
    "c_businesses = reverse_dict(b_cities)\r\n",
    "b_states = {k: g[\"city\"].tolist() for k,g in item_city.groupby(\"item\")}\r\n",
    "s_businesses = reverse_dict(b_states)\r\n",
    "\r\n",
    "b_types = defaultdict(set)\r\n",
    "for index, row in item_category.iterrows():\r\n",
    "    types = list(map(int, row['category'].strip().split(' ')))\r\n",
    "    b_types[row['item']].update(types)\r\n",
    "b_types = dict(b_types)\r\n",
    "t_businesses = reverse_dict(b_types)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'item_city' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9c3e9ec499c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mb_cities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"city\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitem_city\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"item\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mc_businesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreverse_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_cities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mb_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"city\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitem_city\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"item\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ms_businesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreverse_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'item_city' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "sum([len(v) for k, v in b_cities.items()]), sum([len(v) for k, v in b_states.items()])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(34259, 34259)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# get UM in support set and query set \r\n",
    "state = \"meta_training\"\r\n",
    "print(state)\r\n",
    "u_businesses = training_x\r\n",
    "u_businesses_y = training_y\r\n",
    "\r\n",
    "support_u_businesses = {}\r\n",
    "support_u_businesses_y = {}\r\n",
    "query_u_businesses = {}\r\n",
    "query_u_businesses_y = {}\r\n",
    "train_u_businesses = {}\r\n",
    "train_u_businesses_y = {}\r\n",
    "\r\n",
    "for u_id in tqdm(u_businesses):  # each task contains support set and query set\r\n",
    "    seen_movie_len = len(u_businesses[u_id])\r\n",
    "    indices = list(range(seen_movie_len))\r\n",
    "    if seen_movie_len < 13 or seen_movie_len > 100:\r\n",
    "        continue\r\n",
    "    \r\n",
    "    support_u_businesses[u_id] = []\r\n",
    "    support_u_businesses_y[u_id] = []\r\n",
    "    query_u_businesses[u_id] = []\r\n",
    "    query_u_businesses_y[u_id] = []\r\n",
    "    \r\n",
    "    train_u_businesses[u_id]  = []\r\n",
    "    train_u_businesses_y[u_id] = []\r\n",
    "    \r\n",
    "    random.shuffle(indices)\r\n",
    "    tmp_movies = np.array(u_businesses[u_id])\r\n",
    "    tmp_y = np.array(u_businesses_y[u_id])\r\n",
    "    \r\n",
    "    support_u_businesses[u_id] += list(map(int, tmp_movies[indices[:-10]]))\r\n",
    "    support_u_businesses_y[u_id] += list(map(int, tmp_y[indices[:-10]]))\r\n",
    "    query_u_businesses[u_id] += list(map(int, tmp_movies[indices[-10:]]))\r\n",
    "    query_u_businesses_y[u_id] += list(map(int, tmp_y[indices[-10:]]))\r\n",
    "    \r\n",
    "    train_u_businesses[u_id] += u_businesses[u_id]\r\n",
    "    train_u_businesses_y[u_id] += u_businesses_y[u_id]\r\n",
    "    \r\n",
    "\r\n",
    "json.dump(support_u_businesses, open(output_dir+state+'/support_u_businesses.json','w'))\r\n",
    "json.dump(support_u_businesses_y, open(output_dir+state+'/support_u_businesses_y.json','w'))\r\n",
    "json.dump(query_u_businesses, open(output_dir+state+'/query_u_businesses.json','w'))\r\n",
    "json.dump(query_u_businesses_y, open(output_dir+state+'/query_u_businesses_y.json','w'))\r\n",
    "len(support_u_businesses), len(support_u_businesses_y), len(query_u_businesses), len(query_u_businesses_y), len(train_u_businesses), len(train_u_businesses_y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  5%|▌         | 2069/41066 [00:00<00:01, 20680.80it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "meta_training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 41066/41066 [00:01<00:00, 23166.07it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20635, 20635, 20635, 20635, 20635, 20635)"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# get mp data \r\n",
    "print(state)\r\n",
    "\r\n",
    "# u_b_u_businesses = {}\r\n",
    "u_b_c_businesses = {}\r\n",
    "u_b_s_businesses = {}\r\n",
    "# u_b_t_businesses = {}\r\n",
    "\r\n",
    "support_b_users = reverse_dict(support_u_businesses)\r\n",
    "\r\n",
    "for u, bs in tqdm(train_u_businesses.items()):\r\n",
    "#     u_b_u_businesses[u] = []\r\n",
    "    u_b_c_businesses[u] = []\r\n",
    "    u_b_s_businesses[u] = []\r\n",
    "#     u_b_t_businesses[u] = []\r\n",
    "    for b in bs:    \r\n",
    "#         cur_bs = set([b])\r\n",
    "#         if b in support_b_users:  # for meta_training, only support set can be seen!!!\r\n",
    "#             for _u in support_b_users[b]:  #  only include user in training set !!!!\r\n",
    "#                 cur_bs.update(support_u_businesses[_u])  # list        \r\n",
    "#         u_b_u_businesses[u].append(list(cur_bs))\r\n",
    "        \r\n",
    "        cur_bs = set()\r\n",
    "        for _c in b_cities[b]:\r\n",
    "            cur_bs.update(c_businesses[_c])\r\n",
    "        u_b_c_businesses[u].append(list(cur_bs))\r\n",
    "        \r\n",
    "        cur_bs = set()\r\n",
    "        for _s in b_states[b]:\r\n",
    "            cur_bs.update(s_businesses[_s])\r\n",
    "        u_b_s_businesses[u].append(list(cur_bs))\r\n",
    "        \r\n",
    "#         cur_bs = set()\r\n",
    "#         for _t in b_types[b]:\r\n",
    "#             cur_bs.update(t_businesses[_t])\r\n",
    "#         u_b_t_businesses[u].append(list(cur_bs))\r\n",
    "\r\n",
    "# print(len(u_b_u_businesses))\r\n",
    "print(len(u_b_c_businesses))\r\n",
    "print(len(u_b_s_businesses))\r\n",
    "# print(len(u_b_t_businesses))\r\n",
    "\r\n",
    "# json.dump(u_b_u_businesses, open(output_dir+state+'/u_b_u_businesses.json','w'))\r\n",
    "json.dump(u_b_c_businesses, open(output_dir+state+'/u_b_c_businesses.json','w')) \r\n",
    "json.dump(u_b_s_businesses, open(output_dir+state+'/u_b_s_businesses.json','w')) \r\n",
    "# json.dump(u_b_t_businesses, open(output_dir+state+'/u_b_t_businesses.json','w'))\r\n",
    "print('write done!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "meta_training\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20635/20635 [06:21<00:00, 54.11it/s]  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20635\n",
      "20635\n",
      "write done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "len(train_u_businesses[4]) == len(u_b_u_businesses[4]), len(train_u_businesses[4]) == len(u_b_c_businesses[4]), len(train_u_businesses[4]) == len(u_b_s_businesses[4])\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "tqdm._instances.clear()\r\n",
    "del tqdm\r\n",
    "from tqdm import tqdm "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "428it [55:20,  7.76s/it]\n",
      "  1%|▏         | 433/34259 [00:10<14:18, 39.39it/s] \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "if support_u_businesses.keys() == query_u_businesses.keys():\r\n",
    "    u_id_list = support_u_businesses.keys()\r\n",
    "print(len(u_id_list))\r\n",
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\r\n",
    "    support_x_app = None\r\n",
    "    support_x_app_melu = None\r\n",
    "    support_ub_app = []\r\n",
    "    support_ubub_app = []\r\n",
    "    support_ubcb_app = []\r\n",
    "    support_ubsb_app = []\r\n",
    "        \r\n",
    "    for index1, b_id in enumerate(support_u_businesses[u_id]):\r\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\r\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\r\n",
    "        try:\r\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\r\n",
    "            support_x_app_melu = torch.cat((support_x_app_melu, tmp_x_converted_melu), 0)\r\n",
    "        except:\r\n",
    "            support_x_app = tmp_x_converted\r\n",
    "            support_x_app_melu = tmp_x_converted_melu\r\n",
    "\r\n",
    "        # meta-paths\r\n",
    "        # UB\r\n",
    "        support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], support_u_businesses[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\r\n",
    "        # UBUB\r\n",
    "        support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index1])), dim=0))\r\n",
    "        # UBCB\r\n",
    "        support_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index1])), dim=0))\r\n",
    "        # UBSB\r\n",
    "        support_ubsb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_s_businesses[u_id][index1])), dim=0))\r\n",
    "#         # UBTB\r\n",
    "#         support_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][b_id])), dim=0))\r\n",
    "        \r\n",
    "    support_y_app = torch.FloatTensor(support_u_businesses_y[u_id])\r\n",
    "    \r\n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ubcb_app, open(\"{}/{}/support_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ubsb_app, open(\"{}/{}/support_ubsb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    # data for MeLU\r\n",
    "    pickle.dump(support_x_app_melu, open(\"{}/{}/support_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "    \r\n",
    "    query_x_app = None\r\n",
    "    query_x_app_melu = None\r\n",
    "    query_ub_app = []\r\n",
    "    query_ubub_app = []\r\n",
    "    query_ubcb_app = []\r\n",
    "    query_ubsb_app = []\r\n",
    "        \r\n",
    "    for index2, b_id in enumerate(query_u_businesses[u_id]):\r\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\r\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\r\n",
    "        try:\r\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\r\n",
    "            query_x_app_melu = torch.cat((query_x_app_melu, tmp_x_converted_melu), 0)\r\n",
    "        except:\r\n",
    "            query_x_app = tmp_x_converted\r\n",
    "            query_x_app_melu = tmp_x_converted_melu\r\n",
    "\r\n",
    "        # meta-paths\r\n",
    "        # UB\r\n",
    "        query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], support_u_businesses[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\r\n",
    "        # UBUB\r\n",
    "        query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index2])), dim=0))\r\n",
    "        # UBCB\r\n",
    "        query_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index2])), dim=0))\r\n",
    "        # UBSB\r\n",
    "        query_ubsb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_s_businesses[u_id][index2])), dim=0))\r\n",
    "#         # UBTB\r\n",
    "#         query_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][m_id])), dim=0))\r\n",
    "        \r\n",
    "    query_y_app = torch.FloatTensor(query_u_businesses_y[u_id])\r\n",
    "    \r\n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ub_app, open(\"{}/{}/query_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ubcb_app,open(\"{}/{}/query_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ubsb_app,open(\"{}/{}/query_ubsb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    # data for MeLU\r\n",
    "    pickle.dump(query_x_app_melu, open(\"{}/{}/query_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "\r\n",
    "    with open(\"{}/log/{}/supp_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\r\n",
    "        for i, b_id in enumerate(support_u_businesses[u_id]):\r\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id, support_u_businesses_y[u_id][i]))\r\n",
    "                \r\n",
    "    with open(\"{}/log/{}/query_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\r\n",
    "        for i, b_id in enumerate(query_u_businesses[u_id]):\r\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id,  query_u_businesses_y[u_id][i]))\r\n",
    "            \r\n",
    "print(idx)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20635\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "428it [40:27,  2.81s/it]"
     ]
    },
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c3572ebecee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_ubub_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_ubub_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_ubcb_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_ubcb_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_ubsb_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_ubsb_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m# data for MeLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x_app_melu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_x_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelu_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get UM in support set and query set \r\n",
    "# state = \"warm_up\"\r\n",
    "# print(state)\r\n",
    "# u_businesses = warm_x\r\n",
    "# u_businesses_y = warm_y\r\n",
    "\r\n",
    "# state = \"user_cold_testing\"\r\n",
    "# print(state)\r\n",
    "# u_businesses = user_cold_x\r\n",
    "# u_businesses_y = user_cold_y\r\n",
    "\r\n",
    "# state = \"item_cold_testing\"\r\n",
    "# print(state)\r\n",
    "# u_businesses = item_cold_x\r\n",
    "# u_businesses_y = item_cold_y\r\n",
    "\r\n",
    "state = \"user_and_item_cold_testing\"\r\n",
    "print(state)\r\n",
    "u_businesses = user_item_cold_x\r\n",
    "u_businesses_y = user_item_cold_y\r\n",
    "\r\n",
    "support_u_businesses = {}\r\n",
    "support_u_businesses_y = {}\r\n",
    "query_u_businesses = {}\r\n",
    "query_u_businesses_y = {}\r\n",
    "\r\n",
    "cur_training_u_businesses = train_u_businesses\r\n",
    "\r\n",
    "for u_id in tqdm(u_businesses):  # each task contains support set and query set\r\n",
    "    seen_movie_len = len(u_businesses[u_id])\r\n",
    "    indices = list(range(seen_movie_len))\r\n",
    "    if seen_movie_len < 13 or seen_movie_len > 100:\r\n",
    "        continue\r\n",
    "    \r\n",
    "    support_u_businesses[u_id] = []\r\n",
    "    support_u_businesses_y[u_id] = []\r\n",
    "    query_u_businesses[u_id] = []\r\n",
    "    query_u_businesses_y[u_id] = []\r\n",
    "    \r\n",
    "    random.shuffle(indices)\r\n",
    "    tmp_movies = np.array(u_businesses[u_id])\r\n",
    "    tmp_y = np.array(u_businesses_y[u_id])\r\n",
    "    \r\n",
    "    support_u_businesses[u_id] += list(map(int, tmp_movies[indices[:-10]]))\r\n",
    "    support_u_businesses_y[u_id] += list(map(int, tmp_y[indices[:-10]]))\r\n",
    "    query_u_businesses[u_id] += list(map(int, tmp_movies[indices[-10:]]))\r\n",
    "    query_u_businesses_y[u_id] += list(map(int, tmp_y[indices[-10:]]))\r\n",
    "    \r\n",
    "    if u_id in cur_training_u_businesses:\r\n",
    "        cur_training_u_businesses[u_id] += support_u_businesses[u_id]  # based on meat-traing, add the current support set data\r\n",
    "    else:\r\n",
    "        cur_training_u_businesses[u_id] = support_u_businesses[u_id]\r\n",
    "    \r\n",
    "json.dump(support_u_businesses, open(output_dir+state+'/support_u_businesses.json','w'))\r\n",
    "json.dump(support_u_businesses_y, open(output_dir+state+'/support_u_businesses_y.json','w'))\r\n",
    "json.dump(query_u_businesses, open(output_dir+state+'/query_u_businesses.json','w'))\r\n",
    "json.dump(query_u_businesses_y, open(output_dir+state+'/query_u_businesses_y.json','w'))\r\n",
    "\r\n",
    "len(support_u_businesses), len(support_u_businesses_y), len(query_u_businesses), len(query_u_businesses_y), len(cur_training_u_businesses)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get mp data \r\n",
    "print(state)\r\n",
    "\r\n",
    "u_m_u_movies = {}\r\n",
    "u_m_a_movies = {}\r\n",
    "u_m_d_movies = {}\r\n",
    "\r\n",
    "cur_training_m_users = reverse_dict(cur_training_u_movies)\r\n",
    "\r\n",
    "if support_u_movies.keys() == query_u_movies.keys():\r\n",
    "    u_id_list = support_u_movies.keys()\r\n",
    "print(len(u_id_list))\r\n",
    "\r\n",
    "for u in tqdm(u_id_list):\r\n",
    "    u_m_u_movies[u] = {}\r\n",
    "    u_m_a_movies[u] = {}\r\n",
    "    u_m_d_movies[u] = {}\r\n",
    "    for m in support_u_movies[u]:\r\n",
    "        u_m_u_movies[u][m] = [m]   # add itself to avoid empty tensor when build the support set\r\n",
    "        u_m_a_movies[u][m] = []   \r\n",
    "        u_m_d_movies[u][m] = []  \r\n",
    "        \r\n",
    "        if m in cur_training_m_users:  # include users in meta-training  and users  in current support set\r\n",
    "            for _u in cur_training_m_users[m]:  \r\n",
    "                cur_ms = cur_training_u_movies[_u]  # list\r\n",
    "                u_m_u_movies[u][m].extend(cur_ms)\r\n",
    "        u_m_u_movies[u][m] = list(set(u_m_u_movies[u][m]))\r\n",
    "        for _a in m_actors[m]:\r\n",
    "            cur_ms = a_movies[_a]\r\n",
    "            u_m_a_movies[u][m].extend(cur_ms)\r\n",
    "        for _d in m_directors[m]:\r\n",
    "            cur_ms = d_movies[_d]\r\n",
    "            u_m_d_movies[u][m].extend(cur_ms)\r\n",
    "    \r\n",
    "    for m in query_u_movies[u]:\r\n",
    "        u_m_u_movies[u][m] = [m]   # add itself to avoid empty tensor when build the support set\r\n",
    "        u_m_a_movies[u][m] = []   \r\n",
    "        u_m_d_movies[u][m] = []  \r\n",
    "        \r\n",
    "        if m in cur_training_m_users:  # include users in meta-training  and users  in current support set\r\n",
    "            for _u in cur_training_m_users[m]:  \r\n",
    "                cur_ms = cur_training_u_movies[_u]  # list\r\n",
    "                u_m_u_movies[u][m].extend(cur_ms)\r\n",
    "        u_m_u_movies[u][m] = list(set(u_m_u_movies[u][m]))       \r\n",
    "        for _a in m_actors[m]:\r\n",
    "            cur_ms = a_movies[_a]\r\n",
    "            u_m_a_movies[u][m].extend(cur_ms)\r\n",
    "        for _d in m_directors[m]:\r\n",
    "            cur_ms = d_movies[_d]\r\n",
    "            u_m_d_movies[u][m].extend(cur_ms)\r\n",
    "             \r\n",
    "print(len(u_m_u_movies), len(u_m_a_movies), len(u_m_d_movies))\r\n",
    "\r\n",
    "json.dump(u_m_u_movies, open(output_dir+state+'/u_m_u_movies.json','w'))\r\n",
    "json.dump(u_m_a_movies, open(output_dir+state+'/u_m_a_movies.json','w'))\r\n",
    "json.dump(u_m_d_movies, open(output_dir+state+'/u_m_d_movies.json','w')) \r\n",
    "print('write done!')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if support_u_businesses.keys() == query_u_businesses.keys():\r\n",
    "    u_id_list = support_u_businesses.keys()\r\n",
    "print(len(u_id_list))\r\n",
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\r\n",
    "    support_x_app = None\r\n",
    "    support_x_app_melu = None\r\n",
    "    support_ub_app = []\r\n",
    "    support_ubub_app = []\r\n",
    "    support_ubcb_app = []\r\n",
    "    support_ubtb_app = []\r\n",
    "        \r\n",
    "    for b_id in support_u_businesses[u_id]:\r\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\r\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\r\n",
    "        try:\r\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\r\n",
    "            support_x_app_melu = torch.cat((support_x_app_melu, tmp_x_converted_melu), 0)\r\n",
    "        except:\r\n",
    "            support_x_app = tmp_x_converted\r\n",
    "            support_x_app_melu = tmp_x_converted_melu\r\n",
    "\r\n",
    "        # meta-paths\r\n",
    "        # UM\r\n",
    "        support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], cur_training_u_movies[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\r\n",
    "        # UMUM\r\n",
    "        support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][b_id])), dim=0))\r\n",
    "        # UMAM\r\n",
    "        support_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][b_id])), dim=0))\r\n",
    "        # UMDM\r\n",
    "        support_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][b_id])), dim=0))\r\n",
    "        \r\n",
    "    support_y_app = torch.FloatTensor(support_u_businesses_y[u_id])\r\n",
    "    \r\n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ubcb_app, open(\"{}/{}/support_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_ubtb_app, open(\"{}/{}/support_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    # data for MeLU\r\n",
    "    pickle.dump(support_x_app_melu, open(\"{}/{}/support_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "    \r\n",
    "    query_x_app = None\r\n",
    "    query_x_app_melu = None\r\n",
    "    query_ub_app = []\r\n",
    "    query_ubub_app = []\r\n",
    "    query_ubcb_app = []\r\n",
    "    query_ubtb_app = []\r\n",
    "        \r\n",
    "    for b_id in query_u_businesses[u_id]:\r\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\r\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\r\n",
    "        try:\r\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\r\n",
    "            query_x_app_melu = torch.cat((query_x_app_melu, tmp_x_converted_melu), 0)\r\n",
    "        except:\r\n",
    "            query_x_app = tmp_x_converted\r\n",
    "            query_x_app_melu = tmp_x_converted_melu\r\n",
    "\r\n",
    "        # meta-paths\r\n",
    "        # UM\r\n",
    "        query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], cur_training_u_movies[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\r\n",
    "        # UMUM\r\n",
    "        query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][m_id])), dim=0))\r\n",
    "        # UMAM\r\n",
    "        query_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][m_id])), dim=0))\r\n",
    "        # UMDM\r\n",
    "        query_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][m_id])), dim=0))\r\n",
    "        \r\n",
    "    query_y_app = torch.FloatTensor(query_u_businesses_y[u_id])\r\n",
    "    \r\n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ub_app, open(\"{}/{}/query_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ubcb_app,open(\"{}/{}/query_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_ubtb_app,open(\"{}/{}/query_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\r\n",
    "    # data for MeLU\r\n",
    "    pickle.dump(query_x_app_melu, open(\"{}/{}/query_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\r\n",
    "\r\n",
    "    with open(\"{}/log/{}/supp_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\r\n",
    "        for i, b_id in enumerate(support_u_businesses[u_id]):\r\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id, support_u_businesses_y[u_id][i]))\r\n",
    "                \r\n",
    "    with open(\"{}/log/{}/query_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\r\n",
    "        for i, b_id in enumerate(query_u_businesses[u_id]):\r\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id,  query_u_businesses_y[u_id][i]))\r\n",
    "            \r\n",
    "print(idx)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # state = \"user_and_item_cold_testing\"\n",
    "# # u_businesses = user_item_cold_x\n",
    "# # u_businesses_y = user_item_cold_y\n",
    "\n",
    "# # state = \"user_cold_testing\"\n",
    "# # u_businesses = user_cold_x\n",
    "# # u_businesses_y = user_cold_y\n",
    "\n",
    "# # state = \"item_cold_testing\"\n",
    "# # u_businesses = item_cold_x\n",
    "# # u_businesses_y = item_cold_y\n",
    "\n",
    "# # state = \"warm_up\"\n",
    "# # u_businesses = warm_up_x\n",
    "# # u_businesses_y = warm_up_y\n",
    "\n",
    "# state = 'meta_training'\n",
    "# u_businesses = training_x\n",
    "# u_businesses_y = training_y\n",
    "\n",
    "# u_b_u_businesses = {}\n",
    "# u_b_c_businesses = {}\n",
    "# u_b_t_businesses = {}\n",
    "\n",
    "# for u, bs in tqdm(u_businesses.items()):\n",
    "#     u_b_u_businesses[u] = []\n",
    "#     u_b_c_businesses[u] = []\n",
    "#     u_b_t_businesses[u] = []\n",
    "#     for b in bs:\n",
    "#         if b in train_b_users:\n",
    "#             for _u in train_b_users[b]:  #  include user in training set !!!!\n",
    "#                 u_b_u_businesses[u].append(training_x[_u])\n",
    "#         else:\n",
    "#             u_b_u_businesses[u].append([b])  # add itself to avoid empty tensor when build the support set\n",
    "#         for _c in b_cities[b]:\n",
    "#             u_b_c_businesses[u].append(c_businesses[_c])\n",
    "#         for _t in b_types[b]:\n",
    "#             u_b_t_businesses[u].append(t_businesses[_t])\n",
    "        \n",
    "# print(len(u_b_u_businesses), len(u_b_c_businesses), len(u_b_t_businesses))\n",
    "    \n",
    "# np.save(output_dir+state+'/u_b_u_businesses.npy',u_b_u_businesses)\n",
    "# np.save(output_dir+state+'/u_b_c_businesses.npy',u_b_c_businesses)\n",
    "# np.save(output_dir+state+'/u_b_t_businesses.npy',u_b_t_businesses)\n",
    "# # json.dump(u_b_u_businesses, open(output_dir+state+'/u_b_u_businesses.json', 'w'))\n",
    "# # json.dump(u_b_c_businesses, open(output_dir+state+'/u_b_c_businesses.json', 'w'))\n",
    "# # json.dump(u_b_t_businesses, open(output_dir+state+'/u_b_t_businesses.json', 'w'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#     if not os.path.exists(\"{}/log/\".format(output_dir)):\n",
    "#         os.mkdir(\"{}/log/\".format(output_dir))\n",
    "#     if not os.path.exists(\"{}/{}/{}\".format(output_dir, \"log\", state)):\n",
    "#         os.mkdir(\"{}/{}/{}\".format(output_dir, \"log\", state))\n",
    "    \n",
    "#     print(state)\n",
    "#     print(len(u_businesses), len(u_b_u_businesses), len(u_b_c_businesses), len(u_b_t_businesses))\n",
    "#     idx = 0\n",
    "#     for _, u_id in tqdm(enumerate(u_businesses.keys())):  # each task contains support set and query set\n",
    "#         seen_business_len = len(u_businesses[u_id])\n",
    "#         indices = list(range(seen_business_len))\n",
    "        \n",
    "#         if seen_business_len < 13 or seen_business_len > 100:\n",
    "#             continue\n",
    "            \n",
    "#         random.shuffle(indices)\n",
    "#         tmp_businesses = np.array(u_businesses[u_id])\n",
    "#         tmp_y = np.array(u_businesses_y[u_id])\n",
    "\n",
    "#         support_x_app = None\n",
    "#         support_x_app_melu = None\n",
    "#         support_ub_app = []\n",
    "#         support_ubub_app = []\n",
    "#         support_ubcb_app = []\n",
    "#         support_ubtb_app = []\n",
    "#         for index1, b_id in enumerate(tmp_businesses[indices[:-10]]):\n",
    "#             u_id = int(u_id)\n",
    "#             b_id = int(b_id)\n",
    "#             tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "#             tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "#             try:\n",
    "#                 support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "#                 support_x_app_melu = torch.cat((support_x_app_melu, tmp_x_converted_melu), 0)\n",
    "#             except:\n",
    "#                 support_x_app = tmp_x_converted\n",
    "#                 support_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "#             # meta-paths\n",
    "#             # UM\n",
    "#             support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_businesses[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "#             # UMUM\n",
    "#             support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index1])), dim=0))\n",
    "#             # UMAM\n",
    "#             support_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index1])), dim=0))\n",
    "#             # UMDM\n",
    "#             support_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][index1])), dim=0))\n",
    "        \n",
    "#         support_y_app = torch.FloatTensor(tmp_y[indices[:-10]])\n",
    "\n",
    "#         query_x_app = None\n",
    "#         query_x_app_melu = None\n",
    "#         query_ub_app = []\n",
    "#         query_ubub_app = []\n",
    "#         query_ubcb_app = []\n",
    "#         query_ubtb_app = []\n",
    "#         for index2, b_id in enumerate(tmp_businesses[indices[-10:]]):\n",
    "#             u_id = int(u_id)\n",
    "#             b_id = int(b_id)\n",
    "#             tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "#             tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "#             try:\n",
    "#                 query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "#                 query_x_app_melu = torch.cat((query_x_app_melu, tmp_x_converted_melu), 0)\n",
    "#             except:\n",
    "#                 query_x_app = tmp_x_converted\n",
    "#                 query_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "#             # meta-paths\n",
    "#             # UM\n",
    "#             query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_businesses[u_id])), dim=0))\n",
    "#             # UMUM\n",
    "#             query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index2])), dim=0))\n",
    "#             # UMAM\n",
    "#             query_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index2])), dim=0))\n",
    "#             # UMDM\n",
    "#             query_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][index2])), dim=0))\n",
    "\n",
    "#         query_y_app = torch.FloatTensor(tmp_y[indices[-10:]])\n",
    "\n",
    "#         pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ubcb_app, open(\"{}/{}/support_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ubtb_app, open(\"{}/{}/support_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "        \n",
    "#         pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ub_app, open(\"{}/{}/query_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ubcb_app,open(\"{}/{}/query_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ubtb_app,open(\"{}/{}/query_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "        \n",
    "#         # data for MeLU\n",
    "#         pickle.dump(support_x_app_melu, open(\"{}/{}/support_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_x_app_melu, open(\"{}/{}/query_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "\n",
    "#         with open(\"{}/log/{}/supp_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "#             for i, b_id in enumerate(tmp_businesses[indices[:-10]]):\n",
    "#                 f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id, tmp_y[indices[:-10]][i]))\n",
    "#         with open(\"{}/log/{}/query_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "#             for i, b_id in enumerate(tmp_businesses[indices[-10:]]):\n",
    "#                 f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id,  tmp_y[indices[-10:]][i]))\n",
    "#         idx += 1\n",
    "        \n",
    "#     print(idx)  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}